{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.activations import relu="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = '/training.csv'\n",
    "training_data = pd.read_csv(training_file_path) \n",
    "training_data.columns\n",
    "\n",
    "test_file_path = '/testing.csv'\n",
    "test_data = pd.read_csv(test_file_path) \n",
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the training data\n",
    "X_trains = training_data.iloc[:, 1:]\n",
    "y_trains = training_data.iloc[:, 0]\n",
    "\n",
    "# Print the first few rows of the 'X' data\n",
    "print(X_trains.head())\n",
    "# Print the first few values of the 'y' data\n",
    "print(y_trains.head())\n",
    "\n",
    "# Extract the test data\n",
    "X_tests = test_data.iloc[:, 1:]\n",
    "y_tests = test_data.iloc[:, 0]\n",
    "\n",
    "# Print the first few rows of the 'X' data\n",
    "print(X_tests.head())\n",
    "# Print the first few values of the 'y' data\n",
    "print(y_tests.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 2nd and 3rd features\n",
    "feature_2 = X_trains.iloc[:, 1]\n",
    "feature_3 = X_trains.iloc[:, 2]\n",
    "\n",
    "# Get the list of unique labels\n",
    "unique_labels = y_trains.unique()\n",
    "\n",
    "# Assign random colors to each label\n",
    "label_colors = {label: [random.random(), random.random(), random.random()] for label in unique_labels}\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))  # Adjust the plot size\n",
    "\n",
    "# Plot the data points with corresponding colors\n",
    "for label in unique_labels:\n",
    "    label_data = X_trains[y_trains == label]\n",
    "    plt.scatter(label_data.iloc[:, 1], label_data.iloc[:, 2], color=label_colors[label], label=label)\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(title='Label')\n",
    "plt.xlabel('Feature 2')\n",
    "plt.ylabel('Feature 3')\n",
    "plt.title('Scatter plot between Feature 2 and Feature 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of data and percentage of each label\n",
    "total_data = len(y_trains)\n",
    "label_counts = y_trains.value_counts()\n",
    "label_percentages = (label_counts / total_data) * 100\n",
    "\n",
    "# Print the information\n",
    "print(f\"Total number of data: {total_data}\")\n",
    "print(\"Percentage of each label:\")\n",
    "for label, percentage in label_percentages.items():\n",
    "    print(f\"{label}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X\n",
    "X_trains = np.array(X_trains)\n",
    "X_tests = np.array(X_tests)\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the 'y' data\n",
    "label_encoder.fit(y_trains)\n",
    "label_encoder.fit(y_tests)\n",
    "\n",
    "# Transform the 'y' data into encoded values\n",
    "y_trains= label_encoder.transform(y_trains)\n",
    "y_tests = label_encoder.transform(y_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Oversample the minority classes using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "#  Undersample the majority class using RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "#  Apply SMOTE to oversample the minority classes in the training data\n",
    "X_trains, y_trains = smote.fit_resample(X_trains, y_trains)\n",
    "\n",
    "#  Apply RandomUnderSampler to undersample the majority class in the training data\n",
    "X_trains, y_trains = rus.fit_resample(X_trains, y_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RobustScaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit the scaler to the training data\n",
    "scaler.fit(X_trains)\n",
    "\n",
    "# Transform the training data\n",
    "X_trains = scaler.transform(X_trains)\n",
    "\n",
    "# Transform the testing data using the fitted scaler\n",
    "X_tests = scaler.transform(X_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of X_trains and y_trains\n",
    "print(f\"Shape of X_trains: {X_trains.shape}\")\n",
    "print(f\"Shape of y_trains: {y_trains.shape}\")\n",
    "\n",
    "# Data type of X_trains and y_trains\n",
    "print(f\"Data type of X_trains: {X_trains.dtype}\")\n",
    "print(f\"Data type of y_trains: {y_trains.dtype}\")\n",
    "\n",
    "# Print the first 10 rows of X_trains and y_trains\n",
    "print(f\"First 10 rows of X_trains: {X_trains[:10]}\")\n",
    "print(f\"First 10 rows of y_trains: {y_trains[:10]}\")\n",
    "\n",
    "# Shape of X_tests and y_tests\n",
    "print(f\"Shape of X_tests: {X_tests.shape}\")\n",
    "print(f\"Shape of y_tests: {y_tests.shape}\")\n",
    "\n",
    "# Data type of X_tests and y_tests\n",
    "print(f\"Data type of X_tests: {X_tests.dtype}\")\n",
    "print(f\"Data type of y_tests: {y_tests.dtype}\")\n",
    "\n",
    "# Print the first 10 rows of X_tests and y_tests\n",
    "print(f\"First 10 rows of X_tests: {X_tests[:10]}\")\n",
    "print(f\"First 10 rows of y_tests: {y_tests[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) # for consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer((28,)),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.1), name=\"L1\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.1), name=\"L2\"),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.1), name=\"L3\"),\n",
    "        tf.keras.layers.Dense(6, activation=\"linear\", name=\"L4\")\n",
    "    ], name = \"landCover_model\"\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[layer1, layer2, layer3, layer4] = model.layers\n",
    "\n",
    "W1,b1 = layer1.get_weights()\n",
    "W2,b2 = layer2.get_weights()\n",
    "W3,b3 = layer3.get_weights()\n",
    "W4,b4 = layer4.get_weights()\n",
    "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
    "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
    "print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")\n",
    "print(f\"W4 shape = {W4.shape}, b4 shape = {b4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StratifiedKFold object with 5 folds\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "skf.get_n_splits(X_trains, y_trains)\n",
    "\n",
    "# Initialize an empty list to store the loss histories for each fold\n",
    "loss_histories = []\n",
    "\n",
    "fold=0\n",
    "# Loop through each fold\n",
    "for train_index, test_index in skf.split(X_trains, y_trains):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_val = X_trains[train_index], X_trains[test_index]\n",
    "    y_train, y_val = y_trains[train_index], y_trains[test_index]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs = 50, batch_size = 64, validation_data=(X_val, y_val))\n",
    "    loss_histories.append(history.history)\n",
    "\n",
    "    predictions = model.predict(X_val)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calculate precision, recall, and f1-score\n",
    "    precision = precision_score(y_val, predicted_classes, average='macro')\n",
    "    recall = recall_score(y_val, predicted_classes, average='macro')\n",
    "    f1 = f1_score(y_val, predicted_classes, average='macro')\n",
    "\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve for each fold\n",
    "for i, history in enumerate(loss_histories):\n",
    "    # Create a new figure for each fold\n",
    "    plt.figure()\n",
    "    # Plot the training loss for the current fold\n",
    "    plt.plot(history['loss'], label='Training loss')\n",
    "    # Plot the validation loss for the current fold\n",
    "    plt.plot(history['val_loss'], label='Validation loss')\n",
    "    # Set labels and title for the current fold's loss curve\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss Curve for Fold {i+1}')\n",
    "    # Display the legend for the current fold's loss curve\n",
    "    plt.legend()\n",
    "    # Show the loss curve for the current fold\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the overall loss curve\n",
    "plt.figure()\n",
    "epochs = 0  # Keep track of the current epoch\n",
    "for i, history in enumerate(loss_histories):\n",
    "    # Plot the training loss for the current fold\n",
    "    plt.plot(range(epochs, epochs + len(history['loss'])), history['loss'], label=f'Training loss - Fold {i+1}')\n",
    "    # Plot the validation loss for the current fold\n",
    "    plt.plot(range(epochs, epochs + len(history['val_loss'])), history['val_loss'], label=f'Validation loss - Fold {i+1}')\n",
    "    # Update the epoch counter for the next fold\n",
    "    epochs += len(history['loss'])\n",
    "\n",
    "# Set labels and title for the overall loss curve\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Overall Loss Curve for 5 Folds')\n",
    "plt.legend()  # Display the legend for the different folds\n",
    "plt.show()  # Show the overall loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on X_trains\n",
    "predictions = model.predict(X_trains)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_trains, predicted_classes)\n",
    "confusion_mat = confusion_matrix(y_trains, predicted_classes)\n",
    "precision = precision_score(y_trains, predicted_classes, average='macro')\n",
    "recall = recall_score(y_trains, predicted_classes, average='macro')\n",
    "f1 = f1_score(y_trains, predicted_classes, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on X_tests\n",
    "predictions = model.predict(X_tests)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_tests, predicted_classes)\n",
    "confusion_mat = confusion_matrix(y_tests, predicted_classes)\n",
    "precision = precision_score(y_tests, predicted_classes, average='macro')\n",
    "recall = recall_score(y_tests, predicted_classes, average='macro')\n",
    "f1 = f1_score(y_tests, predicted_classes, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
